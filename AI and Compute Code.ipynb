{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare Raw Data</h3>\n",
    "\n",
    "Note: data points through OpenAI Five were provided by OpenAI and have been redacted; data points on language models since then were provided by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = [\n",
    "            # 11 removed items\n",
    "            ['10/11/2018',6.16,'BERT'],\n",
    "            ['7/1/2019',49.3,'RoBERTa'],\n",
    "            ['2/14/2019',31.25,'GPT-2'], #GPT-2 calculated from GPT-3 XL where 27.5 PF-days for 1.32B params converted to 31.25 PF-days for 1.5B params\n",
    "            ['10/23/2019',382,'T5'],\n",
    "            ['5/28/2020', 3.64E3, 'GPT-3'] #GPT-3 paper says 3.64E3 but Nvidia paper says almost 5k.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data so that we have datetime dates and base-2 logs of the compute amount. Also distinguish the OpenAI data from the language model data we have added since then end of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_converted = [[dt.datetime.strptime(val[0], '%m/%d/%Y'), np.log2(val[1])] for val in raw_data]\n",
    "openai_data = data_converted[:-5]\n",
    "notable_llms = data_converted[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all the dates to a raw number representing the number of seconds since 6/23/1912 (an arbitrary date that is 100 years before AlexNet), then calculate separate linear regressions for the period pre- and post-AlexNet. Note that we only care about evaluating the future of the trend originally identified by OpenAI, so we do not include the language models released since their research in our regression lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [val[0] for val in openai_data] \n",
    "y = [val[1] for val in openai_data] \n",
    "\n",
    "basetime = dt.datetime.strptime('6/23/1912','%m/%d/%Y')\n",
    "\n",
    "X_early = [[(val-basetime).total_seconds()] for val in x[0:11]]\n",
    "reg_early = LinearRegression().fit(X_early,y[0:11])\n",
    "y_early_fit = reg_early.predict(X_early)\n",
    "\n",
    "X_late = [[(val-basetime).total_seconds()] for val in x[11:]]\n",
    "reg_late = LinearRegression().fit(X_late,y[11:])\n",
    "y_late_fit = reg_late.predict(X_late)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data to make sure it looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'bo')\n",
    "plt.plot(x[0:11],y_early_fit,'b-')\n",
    "plt.plot(x[11::],y_late_fit,'b-')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Computing Required')\n",
    "plt.title('History of Compute-Intensive Machine Learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Predictor Functions</h3>\n",
    "\n",
    "We want to be able to predict the date at which a certain compute level will be reach, or the compute level that will be reached by a certain date, or the date at which a certain cost will be required, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_compute(date_string):\n",
    "    # Redefine the basetime (same as before)\n",
    "    basetime = dt.datetime.strptime('6/23/1912','%m/%d/%Y')\n",
    "    \n",
    "    # Convert the date string to a datetime object as done for the raw data\n",
    "    eval_dt = dt.datetime.strptime(date_string,'%m/%d/%Y')\n",
    "\n",
    "    # Convert the date into a single number representing seconds since the basetime\n",
    "    diff = (eval_dt-basetime).total_seconds()\n",
    "\n",
    "    # Calculate the predicted compute at that datetime\n",
    "    log_compute = reg_late.predict([[diff]])\n",
    "\n",
    "    # IMPORTANT: remember that this function always returns the base-2 log of the actual compute amount predicted for date_string\n",
    "    return log_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_date(compute):\n",
    "    # Convert the desired compute level to its base-2 log\n",
    "    log_compute = np.log2(compute)\n",
    "\n",
    "    # Calculate the x-coordinate predicted by the post-AlexNet regression line for that level of compute\n",
    "    diff = (log_compute - reg_late.intercept_)/reg_late.coef_\n",
    "\n",
    "    # Convert the x-coordinate to a date and return\n",
    "    date_predicted = basetime+dt.timedelta(seconds=diff[0])\n",
    "\n",
    "    return date_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculations involving the cost of these compute levels, we want to compare predictions under three scenarios: compute/dollar remaining fixed; compute/dollar doubling every 2 years (optimistic scenario); and compute/dollar doubling every 4 years (likely scenario). To do this, we need \"offset\" functions that can reflect changing prices for compute. The following function provides linear regressions for cost curves. It takes a date as an input, where the provided date represents the date at which compute/dollar will have doubled relative to January 1, 2021 (a somewhat arbitrary date representing the point at which we let these curves start diverging). It returns a regression that can be used to offset cost predictions based on changing price information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_curve(new_date):\n",
    "\n",
    "    # Define our initial constants: a Google Cloud TPU v3 cost $8.00/hr on 1/1/2021 and had an advertised top speed of 420 teraFLOPs (i.e. 0.42 petaFLOPs)\n",
    "    tpu3_pflops = .42\n",
    "    tpu3_cost = 8\n",
    "    cost_per_pflops_day = (1E15 * 3600 * 24 * tpu3_cost) / (tpu3_pflops * 1E15 * 3600)\n",
    "\n",
    "    # Create two x-coordinates representing January 1, 2021 and our date at which we expect compute/dollar to have doubled\n",
    "    x0, x1 = (dt.datetime.strptime('1/1/2021','%m/%d/%Y') - basetime).total_seconds(), (dt.datetime.strptime(new_date,'%m/%d/%Y') - basetime).total_seconds()\n",
    "\n",
    "    # Create two y-coordinates representing the base-2 log of the original cost per petaFLOPS-day and the base two log of half that value\n",
    "    y0, y1 = np.log2(cost_per_pflops_day), np.log2(cost_per_pflops_day / 2)\n",
    "\n",
    "    # Return the regression line between these two points in LOG BASE 2 values\n",
    "    return LinearRegression().fit([[x0], [x1]], [y0, y1])\n",
    "\n",
    "# Create two specific cost curves representing our 4-year and 2-year doubling times\n",
    "cost_curve_4 = make_cost_curve('1/1/2025')\n",
    "cost_curve_2 = make_cost_curve('1/1/2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_date_of_cost(cost, offset=None):\n",
    "    # Redefine our constants \n",
    "    tpu3_pflops = .42\n",
    "    tpu3_cost = 8\n",
    "    cost_per_pflops_day = (1E15 * 3600 * 24 * tpu3_cost) / (tpu3_pflops * 1E15 * 3600)\n",
    "\n",
    "    # Find the date at which this cost would be expended assuming a fixed price of compute\n",
    "    date = predict_date(cost / cost_per_pflops_day)\n",
    "\n",
    "    if offset is not None:\n",
    "        \"\"\"We are going to approximate the exact date by updating the expected cost at the date found above to reflect \n",
    "        a decline in prices between January 1, 2021 and that date. We will then recalculate a new expected date of \n",
    "        intersection, update our cost based on a decline in prices in that (shorter) interval, and so on, until the \n",
    "        difference between the cost of a petaFLOPs-day at our last predicted date and our current predicted date is <= $1.00.\"\"\"\n",
    "\n",
    "        cost_diff = np.inf\n",
    "        while cost_diff > 1:\n",
    "\n",
    "            # Taking 2 to the power of our offset.predict function at the new time gets us the cost per petaFLOPS-day that we expect as of that date\n",
    "            new_cost = 2 ** offset.predict(np.array((date - basetime).total_seconds()).reshape(1, -1))[0]\n",
    "\n",
    "            # We then set our new cost_per_pflops_day to this new_cost, and save the difference between the last cost_per_pflops_day and the current one to cost_diff\n",
    "            cost_diff, cost_per_pflops_day = cost_per_pflops_day - new_cost, new_cost\n",
    "\n",
    "            # We can now predict a new date based on the amount of compute that could be purchased by our sticker price at the new level of cost per petaFLOPS-day\n",
    "            date = predict_date(cost / cost_per_pflops_day)\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cost(date_string, offset=None):\n",
    "    # Redefine our constants\n",
    "    tpu3_pflops = .42\n",
    "    tpu3_cost = 8\n",
    "    cost_per_pflops_day = (1E15 * 3600 * 24 * tpu3_cost) / (tpu3_pflops * 1E15 * 3600)\n",
    "\n",
    "    if offset is not None:\n",
    "        # Calculate what the cost per petaFLOPS-day should be as of the date we have inputted\n",
    "        date = dt.datetime.strptime(date_string, '%m/%d/%Y')\n",
    "        cost_per_pflops_day = 2 ** offset.predict(np.array((date - basetime).total_seconds()).reshape(1, -1))[0]\n",
    "\n",
    "    # Calculate the compute usage predicted as of the date we have inputted\n",
    "    compute_level = 2 ** predict_compute(date_string)[0]\n",
    "\n",
    "    # Return our predicted compute usage times the cost per petaFLOPS-day that we predict for our inputted date\n",
    "    return compute_level * cost_per_pflops_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use these functions to predict the date at which certain milestones of expenditures will be reached: \\\\$3.5 billion (cost of the National Ignition Facility), \\\\$13.5 billion (cost of the search for the Higgs Boson), \\\\$450 billion (2.2\\% of U.S. GDP in 2019, representing the amount that was spent annually on the Apollo Project), and \n",
    "\\\\$21.43 trillion (representing U.S. GDP in 2019). For all predictions we compare the base assumption of no change in cost of copute to our \"most likely\" cost estimate of a doubling in compute/dollar every 4 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('National Ignition Facility')\n",
    "print(predict_date_of_cost(3.5E9))\n",
    "print(predict_date_of_cost(3.5E9, offset=cost_curve_4), '\\n')\n",
    "\n",
    "print('Higgs Boson')\n",
    "print(predict_date_of_cost(1.35E10))\n",
    "print(predict_date_of_cost(1.35E10, offset=cost_curve_4), '\\n')\n",
    "\n",
    "print('Apollo Program')\n",
    "print(predict_date_of_cost(4.5E11))\n",
    "print(predict_date_of_cost(4.5E11, offset=cost_curve_4), '\\n')\n",
    "\n",
    "print('2019 U.S. GDP')\n",
    "print(predict_date_of_cost(2.143E13))\n",
    "print(predict_date_of_cost(2.143E13, offset=cost_curve_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Figure 1</h3>\n",
    "\n",
    "Will display the growth in compute demands before and after AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: although most of the calculations are done using log-2 (because we frequently\n",
    "talk about doubling times in the text of the paper), we wanted this graph to be in \n",
    "log-10 (because orders of magnitude are visually easier to digest). So all the plots\n",
    "contain np.log10(2 ** x) to convert any given value from log-2 to log-10.\"\"\"\n",
    "\n",
    "sns.set_style('white')\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, constrained_layout=True, figsize=(10,4.25))\n",
    "sns.despine()\n",
    "\n",
    "c1 = \"#003DA6\"\n",
    "c2 = \"#853A6D\"\n",
    "c3 = \"#7AC4A5\"\n",
    "\n",
    "# Plot all of the data points from OpenAI on the left-hand figure\n",
    "ax0.scatter(x, [np.log10(2 ** i) for i in y], c=c1)\n",
    "\n",
    "# Plot the regression lines matching to the pre- and post-AlexNet eras\n",
    "ax0.plot(x[0:11], np.log10(2 ** y_early_fit), c=c1)\n",
    "ax0.plot(x[11::], np.log10(2 ** y_late_fit), c=c1)\n",
    "\n",
    "# Add labels \n",
    "ax0.set_xlabel('Year', fontsize=14)\n",
    "ax0.set_ylabel('petaFLOPS-days ($10^y$)', fontsize=14)\n",
    "ax0.set_title('Historical Trend', fontsize=16)\n",
    "\n",
    "\n",
    "# Generate x and y coordinates for the large language models (last 5 items in our data_converted list)\n",
    "x_llms = [i[0] for i in notable_llms]\n",
    "y_llms = [np.log10(2 ** i[1]) for i in notable_llms]\n",
    "\n",
    "# Extend the regression line through the time period spanned by these language models, using the raw date strings from our raw_data list\n",
    "pred_y_llms = [np.log10(2 ** predict_compute(val[0])) for val in raw_data[-5:]]\n",
    "\n",
    "# Plot all the post-AlexNet OpenAI data points as blue dots and a corresponding regression line in blue\n",
    "ax1.scatter(x[11:], [np.log10(2 ** i) for i in y[11:]], c=c1, label='Historical Data\\n[OpenAI Research]')\n",
    "ax1.plot(x[11:], np.log10(2 ** y_late_fit), c=c1)\n",
    "\n",
    "# Plot the more recent language models as green squares and an extension of the OpenAI regression line as a dashed green line\n",
    "ax1.scatter(x_llms, y_llms, marker='s', c=c3, label='Recent Large\\nLanguage Models')\n",
    "ax1.plot([x_llms[0], x_llms[-1]], [pred_y_llms[0], pred_y_llms[-1]], c=c3, linestyle='--')\n",
    "\n",
    "# Adjust labelling\n",
    "ax1.set_xlabel('Year', fontsize=14)\n",
    "ax1.legend(loc=4, fontsize=12)\n",
    "ax1.set_title('Trend Since AlexNet', fontsize=16)\n",
    "\n",
    "#plt.savefig('compute_trends.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Figure 2</h3>\n",
    "\n",
    "This figure will cover the time from January 1, 2022 to June 1, 2027. We do not want to show when costs will intersect with static, 2019-level GDP; rather, we want to figure out when they will intersect with a growing GDP. We assuming growth of 3% for U.S. GDP with error shading of 2% on the low end and 5% on the high end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine our cost constants in global scope\n",
    "tpu3_pflops = .42\n",
    "tpu3_cost = 8\n",
    "cost_per_pflops_day = (1E15 * 3600 * 24 * tpu3_cost) / (tpu3_pflops * 1E15 * 3600)\n",
    "\n",
    "# Create 100 x coordinates for interpolation and convert them to seconds since our basetime in order to use the regressions we've previously defined\n",
    "start = (dt.datetime.strptime('1/1/2022','%m/%d/%Y') - basetime).total_seconds()\n",
    "end = (dt.datetime.strptime('6/1/2027','%m/%d/%Y') - basetime).total_seconds()\n",
    "t = np.linspace(start, end, num=100).reshape(100, 1)\n",
    "\n",
    "# Converted back to regular datetime objects for labelling purposes\n",
    "dates_for_labels = [basetime+dt.timedelta(seconds=val[0]) for val in t]\n",
    "\n",
    "# Calculate the compute levels predicted by the OpenAI regression for each of these 100 points in time\n",
    "compute = 2 ** reg_late.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the GDP at any given point in time\n",
    "def calc_gdp(date_in_seconds_since_basetime, growth_rate):\n",
    "\n",
    "    # Define a constant for the U.S. GDP in 2019\n",
    "    us_gdp_2019 = 2.143E13\n",
    "\n",
    "    # Convert the inputted time (which corresponds to a value from our t array) to a regular datetime object\n",
    "    time = basetime + dt.timedelta(seconds=date_in_seconds_since_basetime)\n",
    "\n",
    "    # Convert that datetime object to a number representing seconds since 2019 (since this is when our GDP value diverges)\n",
    "    secs_since_2019 = (time - dt.datetime.strptime('1/1/2020', '%m/%d/%Y')).total_seconds()\n",
    "\n",
    "    # Convert the seconds since 2019 value into a years since 2019 value\n",
    "    years_since_2019 = secs_since_2019 / (3600 * 24 * 365)\n",
    "\n",
    "    # Use the property that GDP_t = GDP_0 * e ^ (rt) ==> ln(GDP_t) = ln(GDP_0) + rt to find ln(GDP_t) \n",
    "    return np.log(us_gdp_2019) + growth_rate * years_since_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a function to figure out when our line will actually cross the GDP level depending on different pricing assumptions. We do this in a naive way: create an array of 5,000 points in time across our range of interest, then for each point, calculate the expected GDP and the extrapolated total cost for training an AI model. Then simply return the first point at which the latter value is greater than the former. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date_of_crossing(offset=False):\n",
    "    # Note that our time values are significantly more granular than is necessary to later plot the function\n",
    "    t = np.linspace(start, end, num=5000).reshape(5000, 1)\n",
    "\n",
    "    # Calculate expected compute utilization at each point in time\n",
    "    compute = 2 ** reg_late.predict(t)\n",
    "\n",
    "    # Create an array for the cost of a petaFLOPS-day of compute at each point in time\n",
    "    if offset==False:\n",
    "        cost_array = np.array([cost_per_pflops_day] * 5000)\n",
    "    else:\n",
    "        cost_array = 2 ** offset.predict(t)\n",
    "\n",
    "    # Create an array representing the projected GDP for each point in time\n",
    "    gdp_projections = np.array([np.e ** calc_gdp(t[i][0], 0.03) for i in range(len(t))])\n",
    "\n",
    "    # Create an array representing whether or not the cost of the AI model is greater than the projected GDP for each point in time\n",
    "    t_masked = compute * cost_array > gdp_projections\n",
    "\n",
    "    # Return the first point in time where this condition evaluates to true, convert it to a datetime object, and return it\n",
    "    return basetime + dt.timedelta(seconds = t[np.where(t_masked==True)[0][0]][0])\n",
    "\n",
    "# Check the expected point at which we cross U.S. GDP under each of our three pricing scenarios\n",
    "print(find_date_of_crossing())\n",
    "print(find_date_of_crossing(cost_curve_4))\n",
    "print(find_date_of_crossing(cost_curve_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our figure itself involves lots of annotations and repetitive lines, so we will use a few custom helper functions. For our linear-scale graph, we want to display the y-axis in \"trillions of dollars,\" so we will divide all of our compute costs and GDP projects by 10 ^ 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lines(ax, linear=False):\n",
    "    # Create arrays representing compute prices under each of our three pricing scenarios\n",
    "    # Recall that our t array as defined in global scope contains 100 unique points in time\n",
    "    cost_array_fixed, cost_array_4, cost_array_2 = np.array([cost_per_pflops_day] * 100), 2 ** cost_curve_4.predict(t), 2 ** cost_curve_2.predict(t)\n",
    "\n",
    "    # Create arrays representing total compute costs, using our array of expecte compute levels defined previously in global scope\n",
    "    compute_upper, compute_mid, compute_lower = compute * cost_array_fixed, compute * cost_array_4, compute * cost_array_2\n",
    "\n",
    "    # Create arrays representing projected GDP levels for each of our three scenarios (2% growth, 3% growth, 5% growth)\n",
    "    gdp_upper, gdp_mid, gdp_lower = np.array([np.e ** calc_gdp(i[0], 0.05) for i in t]), np.array([np.e ** calc_gdp(i[0], 0.03) for i in t]), np.array([np.e ** calc_gdp(i[0], 0.02) for i in t])\n",
    "\n",
    "    # For the graph with a linear-scale axis, divide values by one trillion\n",
    "    if linear:\n",
    "        compute_upper, compute_mid, compute_lower = compute_upper / 1E12, compute_mid / 1E12, compute_lower / 1E12\n",
    "        gdp_upper, gdp_mid, gdp_lower = gdp_upper / 1E12, gdp_mid / 1E12, gdp_lower / 1E12\n",
    "\n",
    "    # Plot the central compute demand line with a solid line and add error shading\n",
    "    ax.plot(dates_for_labels, compute_mid, c=c1, linestyle='-', label='Predicted Cost of\\nLargest AI Model')\n",
    "    ax.fill_between(dates_for_labels, compute_upper, compute_lower, color=c1, alpha=0.1)\n",
    "\n",
    "    # Plot the central GDP line with a solid line and add error shading\n",
    "    ax.plot(dates_for_labels, gdp_mid, linestyle=':', color=c2, label='Predicted U.S. GDP')\n",
    "    ax.fill_between(dates_for_labels, gdp_upper, gdp_lower, color=c2, alpha=0.1)\n",
    "\n",
    "    # Adjust legends\n",
    "    ax.set_xlabel('Year', fontsize=14)\n",
    "    ax.legend(loc=2, edgecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_cost(ax, text, cost, xycoords, linear=False):\n",
    "    # Choose the appropriate y coordinate depending on whether our graph is using a linear scale or not\n",
    "    cost_to_print = cost if linear == False else cost / 1E12\n",
    "\n",
    "    # Annotate the graph using the text provided, with an error pointing to the point defined by (x = the date \n",
    "    # at which we expect to hit a given cost, y = the cost itself) \n",
    "    ax.annotate(\n",
    "      text=text,\n",
    "      xy=(predict_date_of_cost(cost, offset=cost_curve_4), cost_to_print),\n",
    "      xytext=xycoords,\n",
    "      textcoords='axes fraction',\n",
    "      ha='center',\n",
    "      arrowprops=dict(arrowstyle='wedge', color='black', lw=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally make the figure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fig, (ax0, ax1) = plt.subplots(2, 1, constrained_layout=True, figsize=(8, 9))\n",
    "sns.despine()\n",
    "\n",
    "# Plot and adjust parameters for the top figure\n",
    "plot_lines(ax0)\n",
    "ax0.set_ylabel('Dollars (Log Scale)', fontsize=14)\n",
    "ax0.set_yscale('log')\n",
    "ax0.set_ylim(1E8, 2E15)\n",
    "\n",
    "# Add annotations\n",
    "annotate_cost(ax0, 'Cost of the NIF', 3.5E9, (0.15, 0.4))\n",
    "annotate_cost(ax0, 'Cost of the Higgs\\nBoson Search', 1.35E10, (0.6, 0.2))\n",
    "annotate_cost(ax0, '2.2% of GDP (Annual Cost\\nof Apollo Project)', 5.2E11, (0.4, 0.6))\n",
    "\n",
    "# For the GDP annotation, use the specific date found using our find_date_of_crossing function, above\n",
    "ax0.annotate(\n",
    "    'Total U.S. GDP',\n",
    "    xy=(dt.datetime.strptime('11/15/2026', '%m/%d/%Y'), predict_cost('11/15/2026', offset=cost_curve_4)),\n",
    "    xytext=(0.65, 0.85),\n",
    "    textcoords='axes fraction',\n",
    "    ha='center',\n",
    "    arrowprops=dict(arrowstyle='wedge', color='black', lw=1)\n",
    ")\n",
    "\n",
    "\n",
    "# Plot and adjust parameters for the bottom figure\n",
    "plot_lines(ax1, linear=True)\n",
    "ax1.set_ylabel('Trillions of Dollars (Linear Scale)', fontsize=14)\n",
    "ax1.set_ylim(0, 45)\n",
    "\n",
    "# Add annotations\n",
    "annotate_cost(ax1, 'Cost of the NIF', 3.5E9, (0.15, 0.1), linear=True)\n",
    "annotate_cost(ax1, 'Cost of the Higgs\\nBoson Search', 1.35E10, (0.3, 0.2), linear=True)\n",
    "annotate_cost(ax1, '2.2% of GDP (Annual Cost\\nof Apollo Project)', 5.2E11, (0.5, 0.35), linear=True)\n",
    "\n",
    "# For the GDP annotation, use the specific date found using our find_date_of_crossing function, above\n",
    "ax1.annotate(\n",
    "    'Total U.S. GDP',\n",
    "    xy=(dt.datetime.strptime('11/15/2026', '%m/%d/%Y'), predict_cost('11/15/2026', offset=cost_curve_4) / 1E12),\n",
    "    xytext=(0.65, 0.7),\n",
    "    textcoords='axes fraction',\n",
    "    ha='center',\n",
    "    arrowprops=dict(arrowstyle='wedge', color='black', lw=1)\n",
    ")\n",
    "\n",
    "\n",
    "#plt.savefig('gdp_projections.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analyze Use of Available GPUs</h3>\n",
    "\n",
    "In this section we want to ask at what point a model predicted by this trendline would require the use of all accelerators in all cloud datacenters to train continuously for the accelrators' full productive lives. \n",
    "\n",
    "It is hard to find hard numbers for production volumes of accelerators capable of training large models. \n",
    "\n",
    "Apparently Intel says 7% of the 12 million server units worldwide are for deep learning. (https://www.businesswire.com/news/home/20210819005361/en/Global-Data-Center-Accelerator-Market-Forecast-to-2026-Artificial-Intelligence-to-Drive-the-Growth-of-Cloud-Data-Center-Market---ResearchAndMarkets.com)\n",
    "\n",
    "\n",
    "That seems low given that total quarterly GPU production was 123 million units per year with Nvidia supplying 15.23%. (https://www.tomshardware.com/news/jpr-gpu-q2-vendor-share) They're pulling 25%-37% of revenue from servers (https://www.digitimes.com/news/a20200630PD213.html) and GPUs accounted for 85% of the accelerator market (https://www.mynewsdesk.com/brandessence/pressreleases/data-center-accelerator-market-size-2021-cagr-38-dot-7-percent-3112488) or 80.6% of global AI data center processor revenue (https://omdia.tech.informa.com/pr/2021-aug/nvidia-maintains-dominant-position-in-2020-market-for-ai-processors-for-cloud-and-data-center) (Xilinx next with FPGAs then Google TPUs)\n",
    "\n",
    "\n",
    "All discrete GPUs from all vendors (at least Intel, AMD, and Nvidia) were 22M in Q1 2021. (https://www.tomshardware.com/news/mercury-research-gpu-report-q1-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_production = (123E6) * 0.1523 * 0.37 / 0.806 # Calculating from server fraction of all Nvidia GPUs\n",
    "print(4 * quarterly_production) # How many accelerators total per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_life = 3 #years\n",
    "useful_gpu_count = quarterly_production * 4 * useful_life  # How many total accelerators are available to train on at a given time?\n",
    "\n",
    "compute_per_gpu = 0.163  # A rough number for petaflops. (https://arxiv.org/abs/2104.04473)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we estimate that there are useful_gpu_count number of GPUs available to train on at any given time, and for simplicity, we assume they all have the same throughput of 0.163 petaFLOPs per second (which is probably much higher than the actual average value). How many computations can be performed over a three-year period with these resources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_total = (compute_per_gpu * 3600 * 24 * 365 * useful_life) * useful_gpu_count\n",
    "\n",
    "# But now we need to convert this to petaFLOPS-days, not petaFLOPS\n",
    "compute_total = compute_total / (3600 * 24)\n",
    "compute_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_date(compute_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our calculations predict that we will have to use every GPU in all cloud datacenters by the very end of 2025 in order to keep up with the trendline. For safety, let's check which dates would be consistent with being off by an order of magnitude in either direction for our estimate of total accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound_annual_production = predict_date(compute_total / 10)\n",
    "print(lower_bound_annual_production)\n",
    "\n",
    "upper_bound_annual_production = predict_date(compute_total * 10)\n",
    "print(upper_bound_annual_production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generate Figure 3</h3>\n",
    "\n",
    "This figure uses some custom data from a collection of sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data from GPT-3 paper\n",
    "gpt3_params = np.array([125E6, 356E6, 760E6, 1320E6, 2650E6, 6660E6, 12850E6, 174600E6])\n",
    "gpt3_compute = np.array([2.6, 7.42, 15.8, 27.5, 55.2, 139, 268, 3640])\n",
    "\n",
    "# Define data from Nvidia 1-trillion-param estimate (Nvidia says a 1T parameter model would take 84 days running on 3072 GPUs that each turn out 0.163 PF/s. https://arxiv.org/abs/2104.04473)\n",
    "nvidia_params = 1E12\n",
    "nvidia_compute = 84 * 3072 * 0.163\n",
    "\n",
    "# Define the scaling law for the relationship between params and compute as give by OpenAI's Scaling Laws paper\n",
    "def find_compute(n_params):\n",
    "    return (n_params / (1.3E9)) ** (1 / 0.73)\n",
    "\n",
    "# Define the end points of our line illustrating this scaling law\n",
    "params = np.array([1E9, 1E15])\n",
    "compute = find_compute(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.despine()\n",
    "\n",
    "# Plot the line representing the scaling law equation in log base 10\n",
    "ax.plot(np.log10(params), np.log10(compute), c=c1, label='Predicted Minimum\\nCompute', zorder=1)\n",
    "\n",
    "# Plot the GPT-3 data points as green squares\n",
    "ax.scatter(np.log10(gpt3_params), np.log10(gpt3_compute), marker='s', c=c3, label='Actual GPT-3 Compute')\n",
    "\n",
    "# Plot the Nvidia estimate for 1 trillion parameters\n",
    "ax.scatter(np.log10(nvidia_params), np.log10(nvidia_compute), marker='D', color=\"#853A6D\", label='Projected Compute Levels')\n",
    "\n",
    "# Plot two more custom points representing \"GPT-4\" (17.5 trillion parameters) and 100 trillion parameters on the minimum compute line\n",
    "ax.scatter([np.log10(1.75E13), np.log10(1E14)], [np.log10(find_compute(1.75E13)), np.log10(find_compute(1E14))], marker='D', color='#853A6D', zorder=2)\n",
    "\n",
    "# Add annotations with custom offsets for aesthetic appearance\n",
    "ax.annotate(\n",
    "    'Hypothetical GPT-4\\n(17.5 Trillion Parameters)',\n",
    "    xy=(np.log10(1.75E13) + 0.05, np.log10(find_compute(1.75E13)) - 0.3),\n",
    "    xytext=(0.8, 0.45),\n",
    "    textcoords='axes fraction',\n",
    "    ha='center',\n",
    "    arrowprops=dict(arrowstyle='->', color='black', lw=1)\n",
    ")\n",
    "ax.annotate(\n",
    "    '100 Trillion\\nParameters',\n",
    "    xy=(np.log10(1E14) + 0.08, np.log10(find_compute(1E14)) - 0.25),\n",
    "    xytext=(0.9, 0.65),\n",
    "    textcoords='axes fraction',\n",
    "    ha='center',\n",
    "    arrowprops=dict(arrowstyle='->', color='black', lw=1)\n",
    ")\n",
    "ax.annotate(\n",
    "    'Nvidia Estimate for\\n1 Trillion Parameters',\n",
    "    xy=(np.log10(1E12) - 0.1, np.log10(nvidia_compute)),\n",
    "    xytext=(0.5, 0.8),\n",
    "    textcoords='axes fraction',\n",
    "    ha='center',\n",
    "    arrowprops=dict(arrowstyle='-[', color='black', lw=1, connectionstyle='angle3')\n",
    ")\n",
    "\n",
    "# Adjust labelling and aesthetics\n",
    "ax.legend(fontsize=12, edgecolor='white')\n",
    "ax.set_xlabel('Number of Parameters ($10^x$)', fontsize=14)\n",
    "ax.set_ylabel('petaFLOPS-days ($10^y$)', fontsize=14)\n",
    "ax.set_xlim(7.5, 15)\n",
    "ax.set_ylim(0, 8)\n",
    "\n",
    "#plt.savefig('compute_needs.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
